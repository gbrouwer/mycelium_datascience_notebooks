{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "port lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from __future__ import print_function\n",
    "Fetching data, training a classifier\n",
    "In the previous tutorial, we looked at lime in the two class case. In this tutorial, we will use the 20 newsgroups dataset again, but this time using all of the classes.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "# making class names shorter\n",
    "class_names = [x.split('.')[-1] if 'misc' not in x else '.'.join(x.split('.')[-2:]) for x in newsgroups_train.target_names]\n",
    "class_names[3] = 'pc.hardware'\n",
    "class_names[4] = 'mac.hardware'\n",
    "print(','.join(class_names))\n",
    "atheism,graphics,ms-windows.misc,pc.hardware,mac.hardware,x,misc.forsale,autos,motorcycles,baseball,hockey,crypt,electronics,med,space,christian,guns,mideast,politics.misc,religion.misc\n",
    "Again, let's use the tfidf vectorizer, commonly used for text.\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)\n",
    "This time we will use Multinomial Naive Bayes for classification, so that we can make reference to this document.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(alpha=.01)\n",
    "nb.fit(train_vectors, newsgroups_train.target)\n",
    "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
    "pred = nb.predict(test_vectors)\n",
    "sklearn.metrics.f1_score(newsgroups_test.target, pred, average='weighted')\n",
    "0.83501841939981736\n",
    "We see that this classifier achieves a very high F score. The sklearn guide to 20 newsgroups indicates that Multinomial Naive Bayes overfits this dataset by learning irrelevant stuff, such as headers, by looking at the features with highest coefficients for the model in general. We now use lime to explain individual predictions instead.\n",
    "\n",
    "Explaining predictions using lime\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizer, nb)\n",
    "print(c.predict_proba([newsgroups_test.data[0]]).round(3))\n",
    "[[ 0.001  0.01   0.003  0.047  0.006  0.002  0.003  0.521  0.022  0.008\n",
    "   0.025  0.     0.331  0.003  0.006  0.     0.003  0.     0.001  0.009]]\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "Previously, we used the default parameter for label when generating explanation, which works well in the binary case.\n",
    "For the multiclass case, we have to determine for which labels we will get explanations, via the 'labels' parameter.\n",
    "Below, we generate explanations for labels 0 and 17.\n",
    "\n",
    "idx = 1340\n",
    "exp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=6, labels=[0, 17])\n",
    "print('Document id: %d' % idx)\n",
    "print('Predicted class =', class_names[nb.predict(test_vectors[idx]).reshape(1,-1)[0,0]])\n",
    "print('True class: %s' % class_names[newsgroups_test.target[idx]])\n",
    "Document id: 1340\n",
    "Predicted class = atheism\n",
    "True class: atheism\n",
    "Now, we can see the explanations for different labels. Notice that the positive and negative signs are with respect to a particular label - so that words that are negative towards class 0 may be positive towards class 15, and vice versa.\n",
    "\n",
    "print ('Explanation for class %s' % class_names[0])\n",
    "print ('\\n'.join(map(str, exp.as_list(label=0))))\n",
    "print ()\n",
    "print ('Explanation for class %s' % class_names[17])\n",
    "print ('\\n'.join(map(str, exp.as_list(label=17))))\n",
    "Explanation for class atheism\n",
    "(u'Caused', 0.26601045845449439)\n",
    "(u'Rice', 0.13658462676578711)\n",
    "(u'Genocide', 0.13519771973974065)\n",
    "(u'owlnet', -0.092585962025604027)\n",
    "(u'certainty', -0.088001257903975866)\n",
    "(u'Semitic', -0.085734572813977061)\n",
    "\n",
    "Explanation for class mideast\n",
    "(u'fsu', -0.056622666266163954)\n",
    "(u'Luther', -0.051897643027619206)\n",
    "(u'Theism', -0.049640283384298073)\n",
    "(u'jews', 0.037819200983811155)\n",
    "(u'Caused', -0.037513316854574666)\n",
    "(u'Genocide', 0.027357407069969929)\n",
    "Another alternative is to ask LIME to generate labels for the top K classes. This is shown below with K=2.\n",
    "To see which labels have explanations, use the available_labels function.\n",
    "\n",
    "exp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=6, top_labels=2)\n",
    "print(exp.available_labels())\n",
    "[0, 15]\n",
    "Now let's see some the visualization of the explanations. Notice that for each class, the words on the right side on the line are positive, and the words on the left side are negative. Thus, 'Caused' is positive for atheism, but negative for christian.\n",
    "\n",
    "exp.show_in_notebook(text=False)\n",
    "Prediction probabilities\n",
    "0.50\n",
    "atheism\n",
    "0.43\n",
    "christian\n",
    "0.05\n",
    "religion.misc\n",
    "0.02\n",
    "mideast\n",
    "0.00\n",
    "Other\n",
    "NOT atheism\n",
    "atheism\n",
    "Caused\n",
    "0.26\n",
    "Rice\n",
    "0.14\n",
    "Genocide\n",
    "0.13\n",
    "owlnet\n",
    "0.09\n",
    "scri\n",
    "0.09\n",
    "Semitic\n",
    "0.08\n",
    "NOT christian\n",
    "christian\n",
    "Caused\n",
    "0.18\n",
    "Genocide\n",
    "0.09\n",
    "scri\n",
    "0.09\n",
    "owlnet\n",
    "0.09\n",
    "Luther\n",
    "0.08\n",
    "fsu\n",
    "0.08\n",
    "We notice that the classifier is using reasonable words (such as 'Genocide', 'Luther', 'Semitic', etc), as well as unreasonable ones ('Rice', 'owlnet'). Let's zoom in and just look at the explanations for class 'Atheism'.\n",
    "\n",
    "exp.show_in_notebook(text=newsgroups_test.data[idx], labels=(0,))\n",
    "Prediction probabilities\n",
    "0.50\n",
    "atheism\n",
    "0.43\n",
    "christian\n",
    "0.05\n",
    "religion.misc\n",
    "0.02\n",
    "mideast\n",
    "0.00\n",
    "Other\n",
    "NOT atheism\n",
    "atheism\n",
    "Caused\n",
    "0.26\n",
    "Rice\n",
    "0.14\n",
    "Genocide\n",
    "0.13\n",
    "owlnet\n",
    "0.09\n",
    "scri\n",
    "0.09\n",
    "Semitic\n",
    "0.08\n",
    "Text with highlighted words\n",
    "From: conor@owlnet.rice.edu (Conor Frederick Prischmann)\n",
    "Subject: Re: Genocide is Caused by Theism : Evidence?\n",
    "Organization: Rice University\n",
    "Lines: 23\n",
    "\n",
    "In article |C60A0s.DvI@mailer.cc.fsu.edu| dekorte@dirac.scri.fsu.edu (Stephen L. DeKorte) writes:\n",
    "|\n",
    "|I saw a 3 hour show on PBS the other day about the history of the\n",
    "|Jews. Appearently, the Cursades(a religious war agianst the muslilams\n",
    "|in 'the holy land') sparked the widespread persecution of muslilams \n",
    "|and jews in europe. Among the supporters of the persiecution, were none \n",
    "|other than Martin Luther, and the Vatican.\n",
    "|\n",
    "|Later, Hitler would use Luthers writings to justify his own treatment\n",
    "|of the jews.\n",
    "|| Genocide is Caused by Theism : Evidence?\n",
    "\n",
    "Heck, I remember reading a quote of Luther as something like: \"Jews should\n",
    "be shot like deer.\"  And of course much Catholic doctrine for centuries was \n",
    "extremely anti-Semitic.\n",
    "\n",
    "\n",
    "\n",
    "-- \n",
    "\"Are you so sure that your truth and your justice are worth more than the\n",
    "truths and justices of other centuries?\" - Simone de Beauvoir\n",
    "\"Where is there a certainty that rises above all doubt and withstands all\n",
    "critique?\" - Karl Jaspers          Rice University, Will Rice College '96\n",
    "Looking at this example demonstrates that there can be useful signal in the header or quotes that would generalize - i.e., the Subject line. There is also signal that would not generalize (e.g. email addresses and institution names).\n",
    "\n",
    "Explaining predictions without headers, quotes and footers\n",
    "Finally, we follow the suggestion of removing headers, footers and quotes, and explain the same example with the new data.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)\n",
    "nb = MultinomialNB(alpha=.01)\n",
    "nb.fit(train_vectors, newsgroups_train.target)\n",
    "c = make_pipeline(vectorizer, nb)\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "exp = explainer.explain_instance(newsgroups_test.data[idx], c.predict_proba, num_features=6, top_labels=2)\n",
    "print(exp.available_labels())\n",
    "[15, 17]\n",
    "Notice how different the explanations are for the classifier without headers, footers and quotes. The prediction changes, but so do the reasons.\n",
    "\n",
    "exp.show_in_notebook(text=False)\n",
    "Prediction probabilities\n",
    "0.40\n",
    "christian\n",
    "0.38\n",
    "mideast\n",
    "0.10\n",
    "atheism\n",
    "0.06\n",
    "guns\n",
    "0.07\n",
    "Other\n",
    "NOT christian\n",
    "christian\n",
    "Luther\n",
    "0.23\n",
    "deer\n",
    "0.22\n",
    "Catholic\n",
    "0.19\n",
    "Heck\n",
    "0.19\n",
    "doctrine\n",
    "0.16\n",
    "anti\n",
    "0.08\n",
    "NOT mideast\n",
    "mideast\n",
    "Luther\n",
    "0.20\n",
    "deer\n",
    "0.19\n",
    "Semitic\n",
    "0.15\n",
    "Heck\n",
    "0.11\n",
    "Jews\n",
    "0.10\n",
    "Catholic\n",
    "0.10\n",
    "Let's see the explanation with the text for the top class (christian):\n",
    "\n",
    "exp.show_in_notebook(text=newsgroups_test.data[idx], labels=(15,))\n",
    "Prediction probabilities\n",
    "0.40\n",
    "christian\n",
    "0.38\n",
    "mideast\n",
    "0.10\n",
    "atheism\n",
    "0.06\n",
    "guns\n",
    "0.07\n",
    "Other\n",
    "NOT christian\n",
    "christian\n",
    "Luther\n",
    "0.23\n",
    "deer\n",
    "0.22\n",
    "Catholic\n",
    "0.19\n",
    "Heck\n",
    "0.19\n",
    "doctrine\n",
    "0.16\n",
    "anti\n",
    "0.08\n",
    "Text with highlighted words\n",
    "\n",
    "Heck, I remember reading a quote of Luther as something like: \"Jews should\n",
    "be shot like deer.\"  And of course much Catholic doctrine for centuries was \n",
    "extremely anti-Semitic.\n",
    "\n",
    "\n",
    "Notice how short the text became after removing all of that information. One begins to wonder if this version of the dataset is still useful, or if it is better to find another dataset altogether. Could a reasonable classifier detect that this document belongs to the class atheism?\n",
    "\n",
    "Anyway, I hope this illustrated how to use LIME to explain arbitrary classifiers in the multiclass case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
