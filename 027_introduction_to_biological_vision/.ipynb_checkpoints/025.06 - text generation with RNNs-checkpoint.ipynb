{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:08.261025Z",
     "iopub.status.busy": "2022-05-03T11:14:08.260828Z",
     "iopub.status.idle": "2022-05-03T11:14:10.284556Z",
     "shell.execute_reply": "2022-05-03T11:14:10.283846Z"
    },
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:10.515842Z",
     "iopub.status.busy": "2022-05-03T11:14:10.515602Z",
     "iopub.status.idle": "2022-05-03T11:14:10.521336Z",
     "shell.execute_reply": "2022-05-03T11:14:10.520758Z"
    },
    "id": "aavnuByVymwK"
   },
   "outputs": [],
   "source": [
    "path_to_file = '../../data/shakespeare/shakespeare.txt'\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "### Vectorize the text\n",
    "Before training, you need to convert the strings to a numerical representation. The tf.keras.layers.StringLookuplayer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "outputs": [],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "ids = ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uenivzwqsDhp"
   },
   "source": [
    "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use tf.keras.layers.StringLookup(..., invert=True).  Instead of passing the original vocabulary generated with sorted(set(text)) use the get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the UNK tokens is set the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.244473Z",
     "iopub.status.busy": "2022-05-03T11:14:12.244010Z",
     "iopub.status.idle": "2022-05-03T11:14:12.251817Z",
     "shell.execute_reply": "2022-05-03T11:14:12.251224Z"
    },
    "id": "Wd2m3mqkDjRj"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), \n",
    "    invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqTDDxS-s-H8"
   },
   "source": [
    "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.255016Z",
     "iopub.status.busy": "2022-05-03T11:14:12.254548Z",
     "iopub.status.idle": "2022-05-03T11:14:12.259845Z",
     "shell.execute_reply": "2022-05-03T11:14:12.259298Z"
    },
    "id": "c2GCh0ySD44s"
   },
   "outputs": [],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "orig = tf.strings.reduce_join(chars, axis=-1).numpy();\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "### Prediction\n",
    "\n",
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the outputâ€”the following character at each time step. Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
    "\n",
    "### Create training examples and targets\n",
    "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text. For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right. So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\". To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.283397Z",
     "iopub.status.busy": "2022-05-03T11:14:12.282940Z",
     "iopub.status.idle": "2022-05-03T11:14:12.693441Z",
     "shell.execute_reply": "2022-05-03T11:14:12.692846Z"
    },
    "id": "UopbsKi88tm5"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids);\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "# for seq in sequences.take(1):\n",
    "#     print(chars_from_ids(seq))\n",
    "# for seq in sequences.take(1):\n",
    "#     print(text_from_ids(seq).numpy())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data in input and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character. Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.757751Z",
     "iopub.status.busy": "2022-05-03T11:14:12.757363Z",
     "iopub.status.idle": "2022-05-03T11:14:12.760852Z",
     "shell.execute_reply": "2022-05-03T11:14:12.760178Z"
    },
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.770840Z",
     "iopub.status.busy": "2022-05-03T11:14:12.770439Z",
     "iopub.status.idle": "2022-05-03T11:14:12.816930Z",
     "shell.execute_reply": "2022-05-03T11:14:12.816374Z"
    },
    "id": "B9iKPXkw5xwa"
   },
   "source": [
    "### Create Dataset from Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.820172Z",
     "iopub.status.busy": "2022-05-03T11:14:12.819702Z",
     "iopub.status.idle": "2022-05-03T11:14:12.842329Z",
     "shell.execute_reply": "2022-05-03T11:14:12.841738Z"
    },
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "Input : b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "Target: b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "Input : b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
      "Target: b\"ow Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(3):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "You used tf.dat  to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.845656Z",
     "iopub.status.busy": "2022-05-03T11:14:12.845278Z",
     "iopub.status.idle": "2022-05-03T11:14:12.852723Z",
     "shell.execute_reply": "2022-05-03T11:14:12.852173Z"
    },
    "id": "p2pGotuNzf-S"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "#### Input\n",
    "tf.keras.layers.Embedding. The input layer. A trainable lookup table that will map each character-ID to a vector \n",
    "with embedding_dim dimensions;\n",
    "\n",
    "#### Hidden\n",
    "tf.keras.layers.GRU \n",
    "A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
    "\n",
    "#### Output\n",
    "tf.keras.layers.Dense: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.856129Z",
     "iopub.status.busy": "2022-05-03T11:14:12.855554Z",
     "iopub.status.idle": "2022-05-03T11:14:12.860086Z",
     "shell.execute_reply": "2022-05-03T11:14:12.859494Z"
    },
    "id": "zHT8cLh7EAsg"
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.862758Z",
     "iopub.status.busy": "2022-05-03T11:14:12.862561Z",
     "iopub.status.idle": "2022-05-03T11:14:12.868444Z",
     "shell.execute_reply": "2022-05-03T11:14:12.867942Z"
    },
    "id": "wj8HQ2w8z4iO"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "            x, states = self.gru(x, initial_state=states, training=training)\n",
    "            x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.871482Z",
     "iopub.status.busy": "2022-05-03T11:14:12.870964Z",
     "iopub.status.idle": "2022-05-03T11:14:12.884188Z",
     "shell.execute_reply": "2022-05-03T11:14:12.883662Z"
    },
    "id": "IX58Xj9z47Aw"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:12.887337Z",
     "iopub.status.busy": "2022-05-03T11:14:12.887142Z",
     "iopub.status.idle": "2022-05-03T11:14:19.110856Z",
     "shell.execute_reply": "2022-05-03T11:14:19.110010Z"
    },
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:19.114557Z",
     "iopub.status.busy": "2022-05-03T11:14:19.114071Z",
     "iopub.status.idle": "2022-05-03T11:14:19.127462Z",
     "shell.execute_reply": "2022-05-03T11:14:19.126830Z"
    },
    "id": "vPGmAAXmVLGC"
   },
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary. Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop. Try it for the first example in the batch, this gives us, at each timestep, a prediction of the next character index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:19.143371Z",
     "iopub.status.busy": "2022-05-03T11:14:19.143038Z",
     "iopub.status.idle": "2022-05-03T11:14:19.148215Z",
     "shell.execute_reply": "2022-05-03T11:14:19.147418Z"
    },
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\" perfume itself\\nTo whom they go to. What will you read to her?\\n\\nLUCENTIO:\\nWhate'er I read to her, I'\"\n",
      "Next Char Predictions:\n",
      " b\"uLAgl,fBYBB$L'g$ g WHlc-&AEJ:ikOOVCcxPFNLOB3BjPFU!:r-iPRUecJEVoC,Ugxk:hszvKTyMGs::$oUeBdZZdK3aL.Jznd\"\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Loss function\n",
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:19.160907Z",
     "iopub.status.busy": "2022-05-03T11:14:19.160460Z",
     "iopub.status.idle": "2022-05-03T11:14:19.163955Z",
     "shell.execute_reply": "2022-05-03T11:14:19.163262Z"
    },
    "id": "ZOeWdgxNFDXq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.189467, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.98761"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:19.200836Z",
     "iopub.status.busy": "2022-05-03T11:14:19.200416Z",
     "iopub.status.idle": "2022-05-03T11:14:19.204379Z",
     "shell.execute_reply": "2022-05-03T11:14:19.203646Z"
    },
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '../../models/rnn/shakespeare/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training\n",
    "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:14:19.214135Z",
     "iopub.status.busy": "2022-05-03T11:14:19.213597Z",
     "iopub.status.idle": "2022-05-03T11:16:14.167240Z",
     "shell.execute_reply": "2022-05-03T11:16:14.166580Z"
    },
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 321s 2s/step - loss: 2.7448\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 308s 2s/step - loss: 2.0066\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 292s 2s/step - loss: 1.7278\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 286s 2s/step - loss: 1.5632\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 274s 2s/step - loss: 1.4616\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 272s 2s/step - loss: 1.3925\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 277s 2s/step - loss: 1.3394\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 292s 2s/step - loss: 1.2951\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 292s 2s/step - loss: 1.2531\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 291s 2s/step - loss: 1.2136\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 288s 2s/step - loss: 1.1758\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 289s 2s/step - loss: 1.1359\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 285s 2s/step - loss: 1.0941\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 284s 2s/step - loss: 1.0505\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 292s 2s/step - loss: 1.0035\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 297s 2s/step - loss: 0.9545\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 295s 2s/step - loss: 0.9037\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 299s 2s/step - loss: 0.8517\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 301s 2s/step - loss: 0.8021\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 243s 1s/step - loss: 0.7515\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIdQ8c8NvMzV"
   },
   "source": [
    "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it. Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text. The following makes a single step prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:16:14.171458Z",
     "iopub.status.busy": "2022-05-03T11:16:14.170910Z",
     "iopub.status.idle": "2022-05-03T11:16:14.180188Z",
     "shell.execute_reply": "2022-05-03T11:16:14.179593Z"
    },
    "id": "iSBU1tHmlUSs"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        #Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create One Step Model and run it\n",
    "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:16:14.197481Z",
     "iopub.status.busy": "2022-05-03T11:16:14.197261Z",
     "iopub.status.idle": "2022-05-03T11:16:16.999406Z",
     "shell.execute_reply": "2022-05-03T11:16:16.998550Z"
    },
    "id": "ST7PSyk9t1mT"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlUQzwu6EXam"
   },
   "source": [
    "## Export the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-03T11:16:19.892817Z",
     "iopub.status.busy": "2022-05-03T11:16:19.892356Z",
     "iopub.status.idle": "2022-05-03T11:16:25.713605Z",
     "shell.execute_reply": "2022-05-03T11:16:25.712920Z"
    },
    "id": "3Grk32H_CzsC"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39msave(one_step_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../models/one_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, '../../models/one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
